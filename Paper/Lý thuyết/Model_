

## üß© 1Ô∏è‚É£ D·∫•u hi·ªáu nh·∫≠n bi·∫øt overfitting c∆° b·∫£n

| D·∫•u hi·ªáu                                                        | Gi·∫£i th√≠ch                                                   |
| --------------------------------------------------------------- | ------------------------------------------------------------ |
| **Train R¬≤ r·∫•t cao**, nh∆∞ng **Test R¬≤ th·∫•p**                    | M√¥ h√¨nh h·ªçc qu√° k·ªπ d·ªØ li·ªáu hu·∫•n luy·ªán, kh√¥ng kh√°i qu√°t ƒë∆∞·ª£c. |
| **Train RMSE r·∫•t nh·ªè**, nh∆∞ng **Test RMSE l·ªõn**                 | Sai s·ªë tr√™n test cao ‚Üí m√¥ h√¨nh kh·ªõp noise.                   |
| **SHAP ho·∫∑c feature importance bi·∫øn ƒë·ªông m·∫°nh** gi·ªØa train/test | M√¥ h√¨nh ‚Äúƒëo√°n m√≤‚Äù h∆°n l√† h·ªçc pattern ·ªïn ƒë·ªãnh.                |
| **Residual plot c√≥ pattern r√µ r√†ng**                            | Kh√¥ng random ‚Üí m√¥ h√¨nh ch∆∞a ph√π h·ª£p d·ªØ li·ªáu.                 |

---

## ‚öôÔ∏è 2Ô∏è‚É£ C√°ch ki·ªÉm tra nhanh trong code

N·∫øu b·∫°n d√πng h√†m `evaluate_model()` nh∆∞ ·ªü tr√™n, ch·ªâ c·∫ßn nh√¨n **Train_R¬≤ vs Test_R¬≤** l√† bi·∫øt ngay.
Nh∆∞ng ta c√≥ th·ªÉ **th√™m c·∫£nh b√°o t·ª± ƒë·ªông** üëá

```python
def check_overfitting(metrics, threshold=0.15):
    """
    Ki·ªÉm tra m·ª©c ƒë·ªô overfitting d·ª±a tr√™n ch√™nh l·ªách R¬≤ train-test.
    threshold: ng∆∞·ª°ng ch√™nh l·ªách cho ph√©p (th∆∞·ªùng 0.1‚Äì0.2)
    """
    diff = metrics["Train_R2"] - metrics["Test_R2"]
    if diff > threshold:
        print(f"‚ö†Ô∏è Warning: Possible overfitting detected! (ŒîR¬≤ = {diff:.3f})")
    else:
        print(f"‚úÖ Model generalizes well (ŒîR¬≤ = {diff:.3f})")
```

---

## üìä 3Ô∏è‚É£ Tr·ª±c quan ho√° (visual check)

### (a) So s√°nh ph√¢n ph·ªëi y_pred gi·ªØa train & test

```python
import matplotlib.pyplot as plt

def plot_pred_distributions(model, X_train, y_train, X_test, y_test):
    plt.figure(figsize=(7,4))
    plt.hist(model.predict(X_train), bins=30, alpha=0.6, label='Train predictions')
    plt.hist(model.predict(X_test), bins=30, alpha=0.6, label='Test predictions')
    plt.legend()
    plt.title("Distribution of Predictions (Train vs Test)")
    plt.show()
```

‚Üí N·∫øu hai ph√¢n ph·ªëi qu√° kh√°c nhau ‚áí overfit.

---

### (b) Residual Plot

```python
def plot_residuals(model, X_test, y_test):
    y_pred = model.predict(X_test)
    residuals = y_test - y_pred
    plt.figure(figsize=(6,4))
    plt.scatter(y_pred, residuals, alpha=0.5)
    plt.axhline(0, color='red', linestyle='--')
    plt.xlabel("Predicted")
    plt.ylabel("Residuals")
    plt.title("Residual Plot")
    plt.show()
```

‚Üí Residual n√™n **ph√¢n b·ªë ng·∫´u nhi√™n quanh 0**. N·∫øu c√≥ d·∫°ng cong ho·∫∑c l·ªách ‚áí m√¥ h√¨nh ch∆∞a ·ªïn ho·∫∑c overfit.

---

## üß† 4Ô∏è‚É£ Khi ph√°t hi·ªán overfit th√¨ x·ª≠ l√Ω th·∫ø n√†o?

| Gi·∫£i ph√°p                                                              | √Åp d·ª•ng cho       |
| ---------------------------------------------------------------------- | ----------------- |
| **Gi·∫£m ƒë·ªô ph·ª©c t·∫°p** (`max_depth`, `n_estimators`, `min_child_weight`) | XGBoost, RF       |
| **Regularization** (`reg_alpha`, `reg_lambda`, `l1_ratio`)             | XGB, Lasso, Ridge |
| **Cross-validation nghi√™m ng·∫∑t h∆°n** (n_splits ‚Üë)                      | ML model          |
| **Feature selection / PCA / SHAP pruning**                             | Gi·∫£m nhi·ªÖu bi·∫øn   |
| **TƒÉng k√≠ch th∆∞·ªõc d·ªØ li·ªáu** ho·∫∑c rolling window r·ªông h∆°n               | Panel/Time-series |

---

