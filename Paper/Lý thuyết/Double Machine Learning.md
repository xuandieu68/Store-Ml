#  **Double Machine Learning (DML)**

---

## üß† 1. Tr·ª±c gi√°c: DML gi·∫£i quy·∫øt v·∫•n ƒë·ªÅ g√¨?

Trong kinh t·∫ø l∆∞·ª£ng, khi ta mu·ªën **∆∞·ªõc l∆∞·ª£ng t√°c ƒë·ªông nh√¢n qu·∫£ (causal effect)** c·ªßa m·ªôt bi·∫øn ( D ) (ch·∫≥ng h·∫°n leverage) l√™n ( Y ) (firm value), ta th∆∞·ªùng lo ng·∫°i r·∫±ng:

* ( D ) c√≥ th·ªÉ t∆∞∆°ng quan v·ªõi c√°c bi·∫øn nhi·ªÖu ( X ),
* v√† m√¥ h√¨nh phi tuy·∫øn ho·∫∑c t∆∞∆°ng t√°c ph·ª©c t·∫°p khi·∫øn OLS b·ªã **thi√™n l·ªách (biased)**.

üëâ Double Machine Learning ƒë∆∞·ª£c t·∫°o ra ƒë·ªÉ:

> ‚ÄúT√°ch ri√™ng ph·∫ßn nh√¢n qu·∫£ kh·ªèi ph·∫ßn d·ª± ƒëo√°n ph·ª©c t·∫°p c·ªßa machine learning, ƒë·ªÉ ∆∞·ªõc l∆∞·ª£ng hi·ªáu ·ª©ng nh√¢n qu·∫£ ch√≠nh x√°c ngay c·∫£ khi c√≥ nhi·ªÅu bi·∫øn ki·ªÉm so√°t (high-dimensional controls).‚Äù

---

## üß© 2. C·∫•u tr√∫c c∆° b·∫£n c·ªßa DML

Gi·∫£ s·ª≠ ta c√≥ m√¥ h√¨nh nh√¢n qu·∫£:
[
Y = \theta D + g(X) + \varepsilon
]
v√† ( D ) ph·ª• thu·ªôc v√†o ( X ):
[
D = m(X) + \nu
]

Trong ƒë√≥:

* ( Y ): bi·∫øn ph·ª• thu·ªôc (v√≠ d·ª•: firm value),
* ( D ): bi·∫øn x·ª≠ l√Ω (treatment, v√≠ d·ª•: leverage),
* ( X ): t·∫≠p bi·∫øn ki·ªÉm so√°t (v√≠ d·ª•: size, growth, profitability,...),
* ( g(X) ), ( m(X) ): h√†m phi tuy·∫øn, c√≥ th·ªÉ h·ªçc b·∫±ng ML (Random Forest, XGBoost, Lasso,...).

---

## ‚öôÔ∏è 3. C√°c b∆∞·ªõc th·ª±c hi·ªán Double Machine Learning

### üîπ B∆∞·ªõc 1: Cross-fitting (ƒëi·ªÉm m·∫•u ch·ªët)

Chia d·ªØ li·ªáu th√†nh K-fold (th∆∞·ªùng K=2 ho·∫∑c 5).

### üîπ B∆∞·ªõc 2: D·ª± ƒëo√°n ph·∫ßn d∆∞ b·∫±ng ML

V·ªõi t·ª´ng fold:

1. Hu·∫•n luy·ªán m√¥ h√¨nh ML tr√™n t·∫≠p train ƒë·ªÉ ∆∞·ªõc l∆∞·ª£ng ( \hat{g}(X) ) v√† ( \hat{m}(X) ),
2. Tr√™n t·∫≠p test:

   * L·∫•y **ph·∫ßn d∆∞ outcome:**
     [
     \tilde{Y} = Y - \hat{g}(X)
     ]
   * L·∫•y **ph·∫ßn d∆∞ treatment:**
     [
     \tilde{D} = D - \hat{m}(X)
     ]

### üîπ B∆∞·ªõc 3: H·ªìi quy ph·∫ßn d∆∞

Sau ƒë√≥ ∆∞·ªõc l∆∞·ª£ng h·ªá s·ªë nh√¢n qu·∫£:
[
\hat{\theta} = \frac{\text{Cov}(\tilde{Y}, \tilde{D})}{\text{Var}(\tilde{D})}
]
ho·∫∑c ƒë∆°n gi·∫£n l√† OLS c·ªßa (\tilde{Y}) l√™n (\tilde{D}).

---

## üß© 4. V√¨ sao g·ªçi l√† ‚ÄúDouble‚Äù Machine Learning?

B·ªüi v√¨:

* Ta d√πng ML **hai l·∫ßn**: m·ªôt l·∫ßn cho outcome ((Y)) v√† m·ªôt l·∫ßn cho treatment ((D)),
* Nh·∫±m lo·∫°i b·ªè ph·∫ßn phi tuy·∫øn v√† t∆∞∆°ng t√°c ph·ª©c t·∫°p c·ªßa c√°c bi·∫øn ki·ªÉm so√°t.

K·∫øt qu·∫£ l√† ph·∫ßn c√≤n l·∫°i (residuals) ƒë·∫°i di·ªán cho m·ªëi quan h·ªá **nh√¢n qu·∫£ thu·∫ßn t√∫y** gi·ªØa (D) v√† (Y).

---

## üìä 5. ·ª®ng d·ª•ng trong t√†i ch√≠nh

DML r·∫•t h·ªØu √≠ch trong c√°c nghi√™n c·ª©u ki·ªÉu:

* ‚ÄúT√°c ƒë·ªông c·ªßa leverage l√™n firm value‚Äù (v·ªõi nhi·ªÅu bi·∫øn ki·ªÉm so√°t),
* ‚Äú·∫¢nh h∆∞·ªüng c·ªßa ESG score ƒë·∫øn performance‚Äù,
* ‚ÄúCausal effect of liquidity, R&D, or investment on firm growth‚Äù,
* Khi b·∫°n mu·ªën t√°ch bi·ªát ph·∫ßn **causal effect** kh·ªèi ph·∫ßn **predictive noise** trong m√¥ h√¨nh phi tuy·∫øn.

---

## üß© 6. C√¥ng c·ª• v√† th∆∞ vi·ªán ph·ªï bi·∫øn

* **Python:**

  * `EconML` c·ªßa Microsoft (h·ªó tr·ª£ DML, DRLearner, CausalForest, ...),
  * `DoubleML` (th∆∞ vi·ªán chuy√™n cho DML, syntax r·∫•t chu·∫©n).

V√≠ d·ª• (Python ‚Äì DoubleML):

```python
from doubleml import DoubleMLData, DoubleMLPLR
from sklearn.ensemble import RandomForestRegressor

dml_data = DoubleMLData(df, y_col='Y', d_cols='D', x_cols=X_cols)

ml_g = RandomForestRegressor()
ml_m = RandomForestRegressor()

dml_plr = DoubleMLPLR(dml_data, ml_g, ml_m)
dml_plr.fit()

print("Causal effect (theta):", dml_plr.coef)
```

---

## ‚ö†Ô∏è 7. Nh·ªØng ƒëi·ªÅu c·∫ßn l∆∞u √Ω

| Ch·ªß ƒë·ªÅ                               | Gi·∫£i th√≠ch                                                       |
| ------------------------------------ | ---------------------------------------------------------------- |
| **Kh√¥ng thay th·∫ø ML th√¥ng th∆∞·ªùng**   | DML kh√¥ng nh·∫±m d·ª± ƒëo√°n Y, m√† ƒë·ªÉ ∆∞·ªõc l∆∞·ª£ng **hi·ªáu ·ª©ng nh√¢n qu·∫£**. |
| **Cross-fitting l√† b·∫Øt bu·ªôc**        | N·∫øu kh√¥ng chia fold, ∆∞·ªõc l∆∞·ª£ng s·∫Ω b·ªã overfit ‚Üí sai l·ªách.         |
| **Gi·∫£ ƒë·ªãnh unconfoundedness**        | C·∫ßn tin r·∫±ng m·ªçi y·∫øu t·ªë g√¢y nhi·ªÖu ƒë·ªÅu n·∫±m trong X.               |
| **Kh√¥ng ph·∫£i ‚Äúblack box‚Äù ho√†n to√†n** | ML ch·ªâ x·ª≠ l√Ω ph·∫ßn nuisance (g(X), m(X)), ph·∫ßn causal v·∫´n l√† OLS. |
| **Ph·∫£i ƒë·ªß d·ªØ li·ªáu**                  | ML c·∫ßn ƒë·ªß m·∫´u ƒë·ªÉ h·ªçc t·ªët ph·∫ßn nuisance functions.                |

---

## üí° 8. T√≥m t·∫Øt d·ªÖ nh·ªõ

| M·ª•c      | √ù ch√≠nh                                                                   |
| -------- | ------------------------------------------------------------------------- |
| M·ª•c ti√™u | ∆Ø·ªõc l∆∞·ª£ng t√°c ƒë·ªông nh√¢n qu·∫£ trong m√¥i tr∆∞·ªùng nhi·ªÅu bi·∫øn v√† phi tuy·∫øn      |
| √ù t∆∞·ªüng  | D√πng ML ƒë·ªÉ lo·∫°i b·ªè ph·∫ßn ·∫£nh h∆∞·ªüng c·ªßa X tr∆∞·ªõc khi ∆∞·ªõc l∆∞·ª£ng causal effect |
| ‚ÄúDouble‚Äù | ML ƒë∆∞·ª£c d√πng cho c·∫£ outcome v√† treatment                                  |
| L·ª£i √≠ch  | Gi·∫£m bias, v·∫´n cho ph√©p d√πng m√¥ h√¨nh phi tuy·∫øn                            |
| H·∫°n ch·∫ø  | C·∫ßn nhi·ªÅu d·ªØ li·ªáu, kh√¥ng t·ª± ƒë·ªông ki·ªÉm tra gi·∫£ ƒë·ªãnh nh√¢n qu·∫£               |

---
````python
import pandas as pd
import numpy as np
import shap
import statsmodels.api as sm
import matplotlib.pyplot as plt

from doubleml import DoubleMLData, DoubleMLPLR
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import LassoCV, RidgeCV
from sklearn.model_selection import train_test_split

# Optional: for gradient boosting
from xgboost import XGBRegressor
from lightgbm import LGBMRegressor

class DMLUtilityPro:
    def __init__(self, data, y_col, d_col, x_cols,
                 model_name='rf', n_folds=5, random_state=42,
                 ml_g=None, ml_m=None):
        """
        Double Machine Learning Utility Class (Pro version)
        ----------------------------------------------------
        Cho ph√©p linh ho·∫°t ch·ªçn ML model v√† ph√¢n t√≠ch SHAP.
        """

        self.data = data.copy()
        self.y_col = y_col
        self.d_col = d_col
        self.x_cols = x_cols
        self.n_folds = n_folds
        self.random_state = random_state
        self.model_name = model_name.lower()
        self.results = {}

        # === ch·ªçn model ===
        self.ml_g = ml_g or self._get_model()
        self.ml_m = ml_m or self._get_model()

        # === chu·∫©n b·ªã d·ªØ li·ªáu cho DML ===
        self.dml_data = DoubleMLData(self.data, y_col=y_col,
                                     d_cols=d_col, x_cols=x_cols)
        self.dml_model = None
        self.ols_model = None

    # ===========================================================
    # Model selector
    # ===========================================================
    def _get_model(self):
        """Ch·ªçn model theo t√™n"""
        if self.model_name == 'rf':
            return RandomForestRegressor(
                n_estimators=300, max_depth=6, random_state=self.random_state)
        elif self.model_name == 'xgb':
            return XGBRegressor(
                n_estimators=300, learning_rate=0.05,
                max_depth=6, subsample=0.8, colsample_bytree=0.8,
                random_state=self.random_state)
        elif self.model_name == 'lgbm':
            return LGBMRegressor(
                n_estimators=300, learning_rate=0.05,
                max_depth=-1, random_state=self.random_state)
        elif self.model_name == 'lasso':
            return LassoCV(cv=5, random_state=self.random_state)
        elif self.model_name == 'ridge':
            return RidgeCV(cv=5)
        else:
            raise ValueError(f"Model '{self.model_name}' kh√¥ng ƒë∆∞·ª£c h·ªó tr·ª£.")

    # ===========================================================
    # FIT DML
    # ===========================================================
    def fit_dml(self):
        """Hu·∫•n luy·ªán Double Machine Learning"""
        self.dml_model = DoubleMLPLR(self.dml_data, self.ml_g, self.ml_m,
                                     n_folds=self.n_folds)
        self.dml_model.fit()

        coef = self.dml_model.coef[0]
        se = self.dml_model.se[0]
        tval = coef / se
        pval = 2 * (1 - sm.stats.norm.cdf(abs(tval)))

        self.results['DML'] = {
            'coef': coef,
            'se': se,
            't': tval,
            'p': pval
        }
        return self.results['DML']

    # ===========================================================
    # FIT OLS
    # ===========================================================
    def fit_ols(self):
        """H·ªìi quy OLS ƒë·ªÉ so s√°nh"""
        X = self.data[[self.d_col] + self.x_cols]
        X = sm.add_constant(X)
        y = self.data[self.y_col]
        ols = sm.OLS(y, X).fit()

        self.results['OLS'] = {
            'coef': ols.params[self.d_col],
            'se': ols.bse[self.d_col],
            't': ols.tvalues[self.d_col],
            'p': ols.pvalues[self.d_col],
            'r2': ols.rsquared
        }
        self.ols_model = ols
        return self.results['OLS']

    # ===========================================================
    # COMPARE
    # ===========================================================
    def compare(self, round_digits=4):
        """So s√°nh DML vs OLS"""
        if 'DML' not in self.results:
            self.fit_dml()
        if 'OLS' not in self.results:
            self.fit_ols()

        comp = pd.DataFrame([
            {
                'Model': 'OLS',
                'Coef': self.results['OLS']['coef'],
                'SE': self.results['OLS']['se'],
                't': self.results['OLS']['t'],
                'p': self.results['OLS']['p'],
                'R2': self.results['OLS']['r2']
            },
            {
                'Model': 'DML',
                'Coef': self.results['DML']['coef'],
                'SE': self.results['DML']['se'],
                't': self.results['DML']['t'],
                'p': self.results['DML']['p'],
                'R2': np.nan
            }
        ])
        return comp.round(round_digits)

    # ===========================================================
    # SHAP Analysis
    # ===========================================================
    def shap_analysis(self, mode='g', max_display=10):
        """
        Ph√¢n t√≠ch SHAP cho g(X) ho·∫∑c m(X).
        mode = 'g' => Y ~ X
        mode = 'm' => D ~ X
        """
        if mode not in ['g', 'm']:
            raise ValueError("mode ph·∫£i l√† 'g' ho·∫∑c 'm'.")

        # ch·ªçn model v√† target
        target = self.y_col if mode == 'g' else self.d_col
        model = self.ml_g if mode == 'g' else self.ml_m

        X = self.data[self.x_cols]
        y = self.data[target]

        # fit model (v√¨ DoubleML kh√¥ng l∆∞u tr·ª±c ti·∫øp estimator)
        model.fit(X, y)

        explainer = shap.Explainer(model, X)
        shap_values = explainer(X)

        title = f"SHAP Summary for {'g(X): Y~X' if mode=='g' else 'm(X): D~X'} ({self.model_name})"
        shap.summary_plot(shap_values, X, max_display=max_display, show=False)
        plt.title(title)
        plt.tight_layout()
        plt.show()

    # ===========================================================
    # SUMMARY
    # ===========================================================
    def summary(self):
        print("="*65)
        print(f"Double Machine Learning Summary ({self.model_name.upper()})")
        print("="*65)
        print(self.compare())
        print("\nNotes:")
        print("- DML: h·ªçc phi tuy·∫øn g(X), m(X) b·∫±ng ML.")
        print("- OLS: gi·∫£ ƒë·ªãnh tuy·∫øn t√≠nh ho√†n to√†n.")
        print("- SHAP: gi√∫p gi·∫£i th√≠ch h√†m g(X) v√† m(X).")
        print("="*65)



   
````
